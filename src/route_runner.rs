//  streamqueue
//  Â© Copyright 2025, by Marco Mengelkoch
//  Licensed under MIT License, see License file for more details
use crate::config::{Config, Route};
use crate::consumers::{CommitFunc, MessageConsumer};
use crate::deduplication::DeduplicationStore;
use crate::endpoints::{
    create_consumer_from_route, create_dlq_from_route, create_publisher_from_route,
};
use crate::model::CanonicalMessage;
use crate::publishers::MessagePublisher;
use anyhow::Result;
use metrics::{counter, histogram, Counter, Histogram};
use std::path::Path;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::{mpsc, watch, Barrier};
use tokio::task::{JoinHandle, JoinSet};
use tokio::time::sleep;
use tracing::{error, info, instrument, trace};

struct RouteMetrics {
    enabled: bool,
    messages_received: Counter,
    messages_sent: Counter,
    messages_duplicate: Counter,
    messages_dlq: Counter,
    processing_duration: Histogram,
}
impl RouteMetrics {
    pub fn inc_received(&self, count: u64) {
        if self.enabled {
            self.messages_received.increment(count);
        }
    }
    pub fn inc_sent(&self, count: u64) {
        if self.enabled {
            self.messages_sent.increment(count);
        }
    }
    pub fn inc_duplicates(&self, count: u64) {
        if self.enabled {
            self.messages_duplicate.increment(count);
        }
    }
    pub fn inc_dlq(&self, count: u64) {
        if self.enabled {
            self.messages_dlq.increment(count);
        }
    }
    pub fn duration(&self, elapsed: f64) {
        if self.enabled {
            self.processing_duration.record(elapsed);
        }
    }
}

pub(crate) enum RouteRunnerCommand {
    Flush,
}

/// Manages the lifecycle of a single route.
pub(crate) struct RouteRunner {
    name: String,
    route: Route,
    global_config: Config,
    barrier: Arc<Barrier>,
    shutdown_rx: watch::Receiver<()>,
    command_rx: mpsc::Receiver<RouteRunnerCommand>,
}

impl RouteRunner {
    pub(crate) fn new(
        name: String,
        route: Route,
        global_config: Config,
        barrier: Arc<Barrier>,
        shutdown_rx: watch::Receiver<()>,
        command_rx: mpsc::Receiver<RouteRunnerCommand>,
    ) -> Self {
        Self {
            name,
            route,
            global_config,
            barrier,
            shutdown_rx,
            command_rx,
        }
    }

    #[instrument(name = "route", skip_all, fields(route.name = %self.name))]
    pub(crate) async fn run(self) {
        let RouteRunner {
            name,
            route,
            global_config,
            barrier,
            mut shutdown_rx,
            mut command_rx,
        } = self;

        info!("Initializing route {}", name);
        let dedup_store = Self::setup_deduplication(&global_config, &name, &mut shutdown_rx).await;
        // Take ownership of the command receiver. It cannot be cloned.
        loop {
            // Check for shutdown before attempting to connect.
            if shutdown_rx.has_changed().unwrap_or(false) {
                info!("Shutdown signal received during initialization. Route stopping.");
                return;
            }

            // 1. Connect to the publisher (and DLQ) first. Retry on failure.
            let publisher = match Self::connect_with_retry(
                "publisher",
                || create_publisher_from_route(&name, &route.output),
                &mut shutdown_rx,
            )
            .await
            {
                Some(p) => p,
                None => return, // Shutdown was triggered
            };

            // The DLQ is optional, so we handle it separately.
            let dlq_publisher = if route.dlq.is_some() {
                match Self::connect_with_retry(
                    "dlq",
                    || create_dlq_from_route(&route, name.as_str()),
                    &mut shutdown_rx,
                )
                .await
                {
                    Some(Some(d)) => Some(d), // Successfully connected to DLQ
                    Some(None) => None, // Should not happen if dlq is configured, but handle it.
                    None => return,     // Shutdown was triggered during connection attempt.
                }
            } else {
                None // No DLQ configured
            };

            // 2. Connect to the consumer. Retry on failure.
            let consumer = match Self::connect_with_retry(
                "consumer",
                || create_consumer_from_route(&name, &route.input),
                &mut shutdown_rx,
            )
            .await
            {
                Some(c) => c,
                None => return, // Shutdown was triggered
            };

            // Wait for all other routes to also connect their consumers.
            // This prevents fast producers from starting before slow consumers are ready.
            info!("Consumer connected. Waiting for other routes to be ready...");
            barrier.wait().await;

            info!("Route connected. Starting message processing.");
            let route_metrics = Arc::new(RouteMetrics {
                enabled: global_config.metrics.enabled,
                messages_received: counter!("bridge_messages_received_total", "route" => name.clone()),
                messages_sent: counter!("bridge_messages_sent_total", "route" => name.clone()),
                messages_duplicate: counter!("bridge_messages_duplicate_total", "route" => name.clone()),
                messages_dlq: counter!("bridge_messages_dlq_total", "route" => name.clone()),
                processing_duration: histogram!("bridge_message_processing_duration_seconds", "route" => name.clone()),
            });

            let processing_result = Self::process_messages(
                &route,
                &mut shutdown_rx,
                consumer,
                publisher,
                dlq_publisher,
                dedup_store.clone(),
                route_metrics.clone(),
                command_rx,
            )
            .await;

            match processing_result {
                Ok(_) => {
                    info!("Consumer stream ended (e.g., EOF). Route finished.");
                    break; // Exit the loop, route is done.
                }
                Err((e, returned_command_rx)) => {
                    error!(error = %e, "An unrecoverable error occurred during message processing. Re-establishing connections in 5s.");
                    command_rx = returned_command_rx; // Recover ownership for the next loop iteration
                    tokio::select! { _ = sleep(Duration::from_secs(5)) => {}, _ = shutdown_rx.changed() => return }
                }
            }
        }
    }

    /// Connects to an endpoint, retrying with exponential backoff until successful or shutdown.
    async fn connect_with_retry<T, F, Fut>(
        endpoint_name: &str,
        connect_fn: F,
        shutdown_rx: &mut watch::Receiver<()>,
    ) -> Option<T>
    where
        F: Fn() -> Fut,
        Fut: std::future::Future<Output = Result<T>>,
    {
        let mut attempts = 0;
        loop {
            tokio::select! {
                // Clone the receiver to get a mutable copy for `changed()`
                _ = shutdown_rx.changed() => {
                    info!("Shutdown signal received while connecting to {}. Aborting.", endpoint_name);
                    return None;
                }
                result = connect_fn() => {
                    match result {
                        Ok(endpoint) => {
                            info!("Successfully connected to {}", endpoint_name);
                            return Some(endpoint);
                        }
                        Err(e) => {
                            attempts += 1;
                            let backoff = Duration::from_secs(2u64.pow(attempts).min(60));
                            error!(error = %e, attempt = attempts, "Failed to connect to {}. Retrying in {}s...", endpoint_name, backoff.as_secs());
                            sleep(backoff).await;
                        }
                    }
                }
            }
        }
    }

    /// The main loop for receiving, processing, and acknowledging messages.
    #[allow(clippy::too_many_arguments)]
    async fn process_messages(
        route: &Route,
        shutdown_rx: &mut watch::Receiver<()>,
        consumer: Box<dyn MessageConsumer>,
        publisher: Arc<dyn MessagePublisher>,
        dlq_publisher: Option<Arc<dyn MessagePublisher>>,
        dedup_store: Arc<DeduplicationStore>,
        metrics_store: Arc<RouteMetrics>,
        command_rx: mpsc::Receiver<RouteRunnerCommand>,
    ) -> Result<(), (anyhow::Error, mpsc::Receiver<RouteRunnerCommand>)> {
        let concurrency = route.concurrency.unwrap_or(10);

        // Fast path for single-concurrency to avoid channel overhead.
        if concurrency == 1 {
            tracing::debug!("Running in single-concurrency mode (fast path).");
            Self::process_messages_single_threaded(
                route,
                shutdown_rx,
                consumer,
                publisher,
                dlq_publisher,
                dedup_store,
                metrics_store,
                command_rx,
            )
            .await
        } else {
            Self::process_messages_multi_threaded(
                route,
                shutdown_rx,
                consumer,
                publisher,
                dlq_publisher,
                dedup_store,
                metrics_store,
                command_rx,
                concurrency,
            )
            .await
        }
    }

    #[allow(clippy::too_many_arguments)]
    async fn process_messages_multi_threaded(
        route: &Route,
        shutdown_rx: &mut watch::Receiver<()>,
        mut consumer: Box<dyn MessageConsumer>,
        publisher: Arc<dyn MessagePublisher>,
        dlq_publisher: Option<Arc<dyn MessagePublisher>>,
        dedup_store: Arc<DeduplicationStore>,
        metrics_store: Arc<RouteMetrics>,
        mut command_rx: mpsc::Receiver<RouteRunnerCommand>,
        concurrency: usize,
    ) -> Result<(), (anyhow::Error, mpsc::Receiver<RouteRunnerCommand>)> {
        let (work_tx, work_rx) =
            async_channel::bounded::<(CanonicalMessage, CommitFunc)>(concurrency * 2);

        let mut worker_tasks = JoinSet::new();
        let deduplication_enabled = route.deduplication_enabled;

        // --- Spawn Worker Pool ---
        for _ in 0..concurrency {
            let dedup = dedup_store.clone();
            let publisher = publisher.clone();
            let dlq_publisher = dlq_publisher.clone();
            let work_rx = work_rx.clone();
            let metrics_store = metrics_store.clone();

            worker_tasks.spawn(async move {
                while let Ok((message, commit_fn)) = work_rx.recv().await {
                    Self::process_single_message(
                        deduplication_enabled,
                        &metrics_store,
                        &dedup,
                        &publisher,
                        &dlq_publisher,
                        message,
                        commit_fn,
                    )
                    .await;
                }
            });
        }

        // --- Spawn Single Consumer Task ---
        let mut consumer_shutdown_rx = shutdown_rx.clone();
        let mut consumer_task: JoinHandle<Result<(), anyhow::Error>> = tokio::spawn(async move {
            loop {
                let received = tokio::select! {
                    biased;
                    _ = consumer_shutdown_rx.changed() => {
                        info!("Shutdown signal received in consumer task. Exiting.");
                        break;
                    }
                    res = consumer.receive() => res,
                };

                match received {
                    Ok((message, commit_fn)) => {
                        if work_tx.send((message, commit_fn)).await.is_err() {
                            info!("Work channel closed, consumer task stopping.");
                            break;
                        }
                    }
                    Err(e) => {
                        if !e
                            .to_string()
                            .contains("BufferedConsumer channel has been closed")
                        {
                            info!(error = %e, "Underlying consumer failed. Stopping consumer task.");
                        } else {
                            info!("Consumer stream ended (e.g. EOF). Stopping consumer task.");
                        }
                        return Err(e); // Propagate error to signal completion/failure
                    }
                }
            }
            Ok(())
        });

        // --- Main Processing and Command Loop ---
        let mut flush_interval = tokio::time::interval(Duration::from_millis(500));
        'main_loop: loop {
            tokio::select! {
                // Prioritize shutdown signal
                _ = shutdown_rx.changed() => {
                    info!("Shutdown signal received in main loop. Draining route.");
                    break 'main_loop;
                }
                // Handle commands for this route
                Some(command) = command_rx.recv() => {
                    match command {
                        RouteRunnerCommand::Flush => {
                            trace!("Manual flush command received.");
                            Self::flush_publishers(&publisher, &dlq_publisher).await;
                        }
                    }
                },
                // Handle periodic flushing
                _ = flush_interval.tick() => {
                    trace!("Periodic flush triggered.");
                    Self::flush_publishers(&publisher, &dlq_publisher).await;
                },
                // Check if the consumer task has finished (e.g., EOF or error)
                res = &mut consumer_task => {
                    match res {
                        Ok(Ok(())) => info!("Consumer task finished gracefully."),
                        Ok(Err(_)) => info!("Consumer task finished due to stream end or error."),
                        Err(e) => error!(error = %e, "Consumer task panicked!"),
                    }
                    // The consumer is done, so we can stop the main loop.
                    // Workers will finish when the channel is empty.
                    break 'main_loop;
                },
                // Check for worker task completion (should not happen unless there's a panic)
                Some(res) = worker_tasks.join_next() => {
                    match res {
                        Ok(_) => trace!("A worker task finished."),
                        Err(join_err) if join_err.is_cancelled() => {
                            trace!("A task was cancelled.");
                        },
                        Err(join_err) => {
                            error!(error = %join_err, "A route task panicked.");
                            let panic_err = anyhow::anyhow!("A route task panicked: {}", join_err);
                            worker_tasks.abort_all();
                            return Err((panic_err, command_rx));
                        }
                    }
                },
                else => {
                    info!("All route tasks have completed.");
                    break 'main_loop;
                }
            }
        }

        info!("Route processing loop finished. Shutting down workers.");
        consumer_task.abort(); // Ensure consumer task is stopped
        worker_tasks.shutdown().await;

        // After all tasks are done, explicitly flush the publisher to ensure all buffered
        // messages are written before the route runner exits. This is crucial for file-based
        // publishers where the process might exit before the OS flushes the buffer.
        info!("Flushing final messages for route.");
        if let Err(e) = publisher.flush().await {
            return Err((e, command_rx));
        }
        if let Some(dlq) = dlq_publisher {
            if let Err(e) = dlq.flush().await {
                return Err((e, command_rx));
            }
        }
        Ok(())
    }

    #[allow(clippy::too_many_arguments)]
    async fn process_messages_single_threaded(
        route: &Route,
        shutdown_rx: &mut watch::Receiver<()>,
        mut consumer: Box<dyn MessageConsumer>,
        publisher: Arc<dyn MessagePublisher>,
        dlq_publisher: Option<Arc<dyn MessagePublisher>>,
        dedup_store: Arc<DeduplicationStore>,
        metrics_store: Arc<RouteMetrics>,
        mut command_rx: mpsc::Receiver<RouteRunnerCommand>,
    ) -> Result<(), (anyhow::Error, mpsc::Receiver<RouteRunnerCommand>)> {
        let deduplication_enabled = route.deduplication_enabled;
        let mut flush_interval = tokio::time::interval(Duration::from_millis(500));

        'main_loop: loop {
            tokio::select! {
                biased;
                // Prioritize shutdown signal
                _ = shutdown_rx.changed() => {
                    info!("Shutdown signal received in main loop. Draining route.");
                    break 'main_loop;
                }
                // Handle commands for this route
                Some(command) = command_rx.recv() => {
                    match command {
                        RouteRunnerCommand::Flush => {
                            trace!("Manual flush command received.");
                            Self::flush_publishers(&publisher, &dlq_publisher).await;
                        }
                    }
                },
                // Handle periodic flushing
                _ = flush_interval.tick() => {
                    trace!("Periodic flush triggered.");
                    Self::flush_publishers(&publisher, &dlq_publisher).await;
                },
                // Receive and process messages directly
                received = consumer.receive() => {
                    match received {
                        Ok((message, commit_fn)) => {
                            Self::process_single_message(
                                deduplication_enabled,
                                &metrics_store,
                                &dedup_store,
                                &publisher,
                                &dlq_publisher,
                                message,
                                commit_fn,
                            ).await;
                        }
                        Err(e) => {
                            if !e.to_string().contains("BufferedConsumer channel has been closed") {
                                info!(error = %e, "Underlying consumer failed. Stopping consumer task.");
                            } else {
                                info!("Consumer stream ended (e.g. EOF). Stopping consumer task.");
                            }
                            break 'main_loop; // Consumer finished, exit loop.
                        }
                    }
                }
            }
        }

        info!("Route processing loop finished. Flushing final messages.");
        // After the loop, explicitly flush the publisher to ensure all buffered
        // messages are written before the route runner exits.
        if let Err(e) = publisher.flush().await {
            return Err((e, command_rx));
        }
        if let Some(dlq) = dlq_publisher {
            if let Err(e) = dlq.flush().await {
                return Err((e, command_rx));
            }
        }

        Ok(())
    }

    #[allow(clippy::too_many_arguments)]
    async fn process_single_message(
        deduplication_enabled: bool,
        metrics_store: &Arc<RouteMetrics>,
        dedup: &Arc<DeduplicationStore>,
        publisher: &Arc<dyn MessagePublisher>,
        dlq_publisher: &Option<Arc<dyn MessagePublisher>>,
        message: CanonicalMessage,
        commit_fn: CommitFunc,
    ) {
        let msg_id = message.message_id;
        let start_time = std::time::Instant::now();

        if deduplication_enabled && dedup.is_duplicate(&msg_id).unwrap_or(false) {
            trace!(%msg_id, "Duplicate message, skipping.");
            metrics_store.inc_duplicates(1);
            commit_fn(None).await; // Acknowledge the duplicate
            return;
        }
        metrics_store.inc_received(1);

        trace!(%msg_id, "Sending message to publisher.");
        match publisher.send(message.clone()).await {
            Ok(response) => {
                metrics_store.duration(start_time.elapsed().as_secs_f64());
                metrics_store.inc_sent(1);
                trace!(%msg_id, "Message sent successfully.");
                commit_fn(response).await;
            }
            Err(e) => {
                error!(%msg_id, error = %e, "Failed to send message, attempting DLQ.");
                metrics_store.inc_dlq(1);
                if let Some(dlq) = dlq_publisher {
                    if let Err(dlq_err) = dlq.send(message).await {
                        error!(%msg_id, error = %dlq_err, "Failed to send message to DLQ.");
                    }
                }
                commit_fn(None).await; // Acknowledge after DLQ attempt
            }
        }
    }

    async fn flush_publishers(
        publisher: &Arc<dyn MessagePublisher>,
        dlq_publisher: &Option<Arc<dyn MessagePublisher>>,
    ) {
        if let Err(e) = publisher.flush().await {
            error!(error = %e, "Failed to flush publisher");
        }
        if let Some(dlq) = dlq_publisher {
            if let Err(e) = dlq.flush().await {
                error!(error = %e, "Failed to flush DLQ publisher");
            }
        }
    }

    async fn setup_deduplication(
        global_config: &Config,
        name: &str,
        shutdown_rx: &mut watch::Receiver<()>,
    ) -> Arc<DeduplicationStore> {
        let db_path = Path::new(&global_config.sled_path).join(name);
        let dedup_store = Arc::new(
            DeduplicationStore::new(db_path, global_config.dedup_ttl_seconds)
                .expect("Failed to create instance-specific deduplication store"),
        );

        // Spawn a task to periodically flush the deduplication database
        let dedup_flusher = dedup_store.clone();
        let mut shutdown_rx = shutdown_rx.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(10));
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        if let Err(e) = dedup_flusher.flush_async().await {
                            error!(error = %e, "Failed to flush deduplication store");
                        }
                    }
                    _ = shutdown_rx.changed() => {
                        info!("Shutdown signal received, performing final deduplication flush.");
                        if let Err(e) = dedup_flusher.flush_async().await {
                             error!(error = %e, "Failed to perform final flush of deduplication store");
                        }
                        break;
                    }
                }
            }
        });

        dedup_store
    }
}
